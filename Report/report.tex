\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}

\begin{document}
\title{Machine Learning - Project 1}
\author{
  Arnaud Pannatier - 
  Bastian Nanchen - 
  Matteo Giorla\\
  \textit{EPFL}
}
\maketitle

\begin{abstract}
  A critical part of scientific discovery is the
  communication of research findings to peers or the general public.
  Mastery of the process of scientific communication improves the
  visibility and impact of research. While this guide is a necessary
  tool for learning how to write in a manner suitable for publication
  at a scientific venue, it is by no means sufficient, on its own, to
  make its reader an accomplished writer. 
  This guide should be a starting point for further development of 
  writing skills.
\end{abstract}

\section{Introduction}

All the data measured at the CERN the last three years takes about 100 petaoctets \cite{CERN}. To treat this huge amount of data, state of the art algorithms are required. Machine Learning methods are particullarly well suited for this kind of problems.

\section{The Structure of a Paper}

\begin{description}
\item[ML Methods seen in Class] \ \\
  An explanation on the way we implemented the 6 traditional machine learning methods.
\item[Our approach] \ \\
  A descritption of the different steps from the raw data to the results.
\item[Results] \ \\
  A short comment on the obtained results and the difficulties we encountered.
\end{description}

\section{ML Methods seen in Class}

\subsection{Linear regression using gradient descent}
To achieve good linear regression, its loss function must be minimized. In this case the loss function considered is the mean square error (MSE). To minimize it the gradient descent process is applied : its an iterative process that improve at each step the weight vector by stepping in the opposite direction of the computed gradient. 

\subsection{Linear regression using stochastic gradient descent}
The same principle as in the first method is used, but instead of computing the gradient at each step, which is very expensive, we move forward multiple steps by selecting an approximation of the gradient which is computing using a random batch of indices.

\subsection{Least squares regression using normal equations}
Another way of computing the optimum of the loss function is to solve it analytically. It is not always possible but it works well with linear regression using mean square error : the process of optimization becomes in this case equivalent to solve a linear system of equation called \textit{normal equations}. In some cases, the implementation of these methods causes some troubles : if some of the columns are nearly colinear, the matrix of this system is ill-conditioned and leads to big numerical error. To take account of this phenomenon some preprocessing of the data is applied and the singular matrix error are treated differently. 

\subsection{Ridge regression using normal equations}
In order to have a more simpler and meaningful model, it's convenient to penalized the complicated one's directly in the cost function. This is done by adding a a \textit{regularization} term $\lambda \vert\vert w \vert\vert ^2$. Fortunately \textit{normal equations} of this particular problem can be computed as well. This is \textit{ridge regression}.

\subsection{Logistic regression using gradient descent}
The loss function \textit{MSE} is in theory not very well suited for binary classification. To recall the process, in binary classification the model gives prediction that are quantified to two value using a treshold process. As the predicted value can become very large before the quantization, they contribute a lot more to the square loss error than their final value. To take account of these considerations, the \textit{logistic function} is used to transform the prediction into value between 0 and 1. Naturally this transformation will induce an other loss function and therefore an new gradient function. But then the gradient descent process is used to find the optimal weights. In some case, the value of the weights can be large so computing the exponential leads to overflow error in the \textit{logistic function}. In order to solve this problem, the equivalent function : 
\begin{equation}
f(x) = \frac{1}{2}+\frac{1}{2}\tanh \left(\frac{x}{2}\right)
\end{equation} 
is used.
\subsection{Regularized logistic regression using gradient descent}

As in the case of \textit{ridge regression}, it's often more meaningful to have smaller value for the weights. To take this in account a \textit{regularization parameter} $\lambda \vert\vert w \vert\vert ^2$ is added to the loss function. It's gradient is the computed and the gradient descent process is used for the optimization.

\section{Our Approach}

\subsection{Preprocessing and data cleaning}
We tried to clean up the data by removing the meaningless variables (whose value is -999). To do so we first noticed that the data could be clustered into 4 groups defined by their \texttt{PRI\_jet\_num} value (which can be 0, 1, 2 or 3). This attribute defines how many "jet of particles" were detected by the collider. For each of the groups the same attributes are set to -999, since for example, if only one jet was detected the data of the second jet would logically be meaningless. After grouping the data into these 4 clusters we removed collinear columns since they would not have any incidence on the regression and would even get in our way when trying to compute the least squares and ridge regressions.

We tried using standardization of the columns but it appeared that it gave us slightly worse results.

\subsection{Cross-validation and better evaluation function}
In order to see the improvement, the cross-validation process was employed. In this way, we could see if at some point the data was overfitted by the model. To get a more meaningful evaluation function of the model, we created a function that return the percentage of good predicted entries. Combine with cross-validation, this tools really helped finding what was our best model.

\subsection{Feature engineering}
To get better results, we had the idea to fit the features not just linearly but with higher degree polynomials. To achieve this purpose, we extend the data by adding multiple times the data matrix at increasing powers. The risk was at certain point to overfit the data, but we could see this limit by using the cross validation. 

\subsection{Model comparison}
The different methods were used to get prediction from the data. Their efficency were evaluate using the evaluation function. Only two given adequate results : \textit{least square} and \textit{ridge regression}. Sadly the four other needed fine tuning in order to work well and even with that they did not improve the results. 

\section{Results}


\section{Conclusion}
Machine Learning methods coupled with featuring engineering have given outstanding results as more of 80\% of the entries have been well categorized. In particular \textit{ridge regression} was well suited for this set of data, and the better improvment in the categorization process was observed when the data were fitted with polynomials.


\begin{thebibliography}{9}
\bibitem{CERN}
Cian O'Luanaigh, \textit{Le centre de données du CERN franchit les 100 pétaoctets} \\
consulted the 28.10.17 at \url{https://home.cern/fr/about/updates/2013/02/cern-data-centre-passes-100-petabytes}
\bibitem{Course}
Pr. Martin Jaggi and Ruediger Urbanke \textit{Machine Learning course} \\
Given at EPFL, 2017 \\
 \url{https://mlo.epfl.ch/page-146520.html}
\end{thebibliography}


\end{document}


