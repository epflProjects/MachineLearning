\section{Discussion}
\label{sec:discussion}

Here we have presented several methods of data pre-processing to create features from word-embeddings, and subsequently used these features to train a number of models. Empirically, we found that three different word-embeddings -- namely GloVe, skip-gram, and sentiment-specific -- gave comparable performance in the context of a Tweet sentiment classification task.

Classification performance was evaluated using four learning models: logistic regression, support vector machine, neural network, and convolutional neural network (CNN). Of these, all but the CNN took input in the form of word-embeddings which were aggregated per Tweet to create a so-called ``Tweet-embedding'' representation. The CNN accepted pre-trained word-embeddings that had not undergone such transformation, and in fact, achieved the best classification performance on an unseen test dataset.

There are many models for text classification outside of those explored here. Ultimately, classification performance on this task likely could be improved by using a pre-trained model (\emph{e.g.} Facebook's \texttt{fastText} \cite{joulin2016bag}), however, using a black box solution to maximise performance was not the aim of this project. When faced with similar tasks in the future, it would be illustrative to evaluate some of these ``out-of-the-box'' solutions, and explore their implementation details.
