\section{Feature processing}
\label{sec:processing}
In order to train learning models using the Twitter data, it was necessary to perform pre-processing to create useful features that characterise the two classes we seek to disambiguate. To this end, we employed pre-processing steps that broadly fit into three categories: Tweet transformation, transformation to word-embeddings, and transformation to ``Tweet-embeddings''.

\subsection{Tweet transformation}
The dataset that was provided had already undergone some basic pre-processing, but additional transformations were performed in order to improve our classification performance. The following transformations were inspired by the GloVe documentation \cite{glove}.

\subsubsection{Base transformations}
Each line of the provided data files represents one Tweet, with all words separated by a single whitespace. All URLs have been removed, and mentions of other users are replaced by the tag \texttt{<user>}. Necessarily, all emoticons have also been removed, however there is no indication of their presence within the original Tweet.

\subsubsection{Number transformation}
The idea is that all the numbers contained in a tweet have the same meaning for sentiment analysis. Therefore it makes sense to group them. To do that all the numbers are replaced with a tag: \texttt{<NUMBER>}

\subsubsection{Hashtag splitting}
Hashtags are a challenge to process in meaningful ways. A naive approach is to treat unique hashtags as different words in the vocabulary. Alternatively, if the hashtags can be split in an intelligent way, we can add signifcant meaning for the model.  Here, we replace the \# by the tag \texttt{<HASHTAG>} and then split all the remaining words on uppercase letters. In the event the hashtag is in all uppercase, the characters are not split, the words are left as is, and an additional tag \texttt{<ALLCAPS>} is added. For example the hashtag \texttt{\#ILikeMachineLearning} will be transformed to \texttt{<HASHTAG> I Like Machine Learning} and \texttt{\#ILIKEMACHINELEARNING} will be transformed to \texttt{<HASHTAG> ILIKEMACHINELEARNING <ALLCAPS>}. 

\subsubsection{Punctuation repetitions}
Some Tweets contain punctuation that is repeated at the end of a word. As before, it is more convenient to group similar expressions: \emph{i.e.} treat \texttt{!!!!!} the same way \texttt{!!!}, rather than treat them as two differents words. Therefore, each time a punctuation mark is repeated, it is transformed to \texttt{punctuation mark <REPEAT>}. 

\subsubsection{Elongated words}
In some tweets the last letter of some words is repeated. This is a similar problem than before. All these expressions should be treated as the same word in the vocabulary. In order to do so, all the repeated letters at the end of a word are replaced by a tag \texttt{<ELONG>}.

\subsubsection{Contractions and casing}
A final text processing step step is applied, where blank space is used to delimit contractions (\emph{i.e.} \texttt{'ll, 're, 'm, ...}) and and punctuation marks. In addition, all text is set to lowercase, in order to ensure that all phrases are evaluated equally regardless of casing during training.

\subsection{Word embeddings}
After the Tweets are modified in their original text format, it is necessary to transform them to a numerical format. Several methods exist for achieving this, \emph{e.g.} bag of words or $n$-gram representations. Recently, so-called ``word embedding'' formats \cite{} have been employed to great success in next classification tasks. In short, word embeddings map words to high-dimensional vectors that seek to characterise the words' usage in various semantic contexts. \cite{} Examples of their use in recent literature are manifold \cite{} and thus we investigated several variations of this representation.

Generally speaking, the advantage of using word embeddings, is that they allow all words in the vocabulary to be represented by vectors of uniform length. \emph{i.e.} given a vocabulary of $V$ words, and embeddings of dimensionality $D$, we create a $V \times D$ matrix of word embeddings that can be processed further.

\subsubsection{word2vec}
\texttt{word2vec} is a popular software tool used to create state-of-the-art word embeddings. \cite{mikolov2013distributed} It provides two embedding models: ``skip-gram'', and ``continuous bag of words'' (CBOW). The skip-gram model uses the current word of interest as input to a classifier that seeks to predict words within a certain range before or after it. The CBOW model does the opposite, by predicting the current word based on its textual context. The skip-gram architecture was found to perform better for semantic classification tasks, whereas CBOW performed better on syntactic classification. \cite{mikolov2013efficient} Given the task at hand, only skip-gram embeddings were evaluated in this task.

\subsubsection{GloVe}
GloVe (Global Vectors) is a word vector representation that was developed in the wake of \texttt{word2vec}'s popularity. GloVe creates word vectors via factorisation of a word co-occurrence matrix. At a high level, the GloVe algorithm works in three steps: computing co-occurence probabilities for all words $V$ in the vocabulary, and ratios of these probabilities with respect to other context words; applying some soft constraints upon word pairs in the form of linear biases; and computing the cost function with a weighting to prevent learning only from common word pairs. \cite{pennington2014glove}. The principal difference between GloVe and \texttt{word2vec} is that the first method use count-based occurence approach and the second used context-predictions to learn the vectors embeddings. 

\subsubsection{Sentiment-specific word embeddings}
The sentiment-specific word embedding (SSWE) method presented in \cite{tang2014learning} actually takes embeddings generated by other means as input, and modifies them in order to better capture the relative sentiment of words in the vocabulary. As an example, the SSWE representation seeks to represent the words ``good'' and ``bad'' as polar opposites---whereas methods such as GloVe or \texttt{word2vec} would map these words closely, as they are used in syntactically similar contexts. This transformation of embeddings is performed using a four layer neural network, and has been demonstrated as being more effective than standard word embeddings for the specific task of Tweet sentiment classification. \cite{tang2014learning}

\subsection{Tweet-embeddings}
Word-embedding representations create matrices of dimensionality $V \times D$ for $V$ words in a vocabulary, and $D$ elements per word vector. Tweets necessarily contain subsets of words in the vocabulary, and thus we must aggregate the embeddings for each subset of words, in order to construct an $N \times D$ feature matrix for $N$ tweets. To this end we trialled summation, and averaging of vectors to create a ``Tweet-embedding'' representation. In addition, weighted word-embeddings -- where the vector elements are inversely weighted according to frequency -- were evaluated, but the effects were negligible. \cite{schnabel2015evaluation} Finally, the mean of the word-embeddings for each Tweet was used.
